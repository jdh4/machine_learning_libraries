# NVIDIA Rapids

Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.

cuDF provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.

cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.

cuML enables data scientists, researchers, and software engineers to run traditional tabular ML tasks on GPUs without going into the details of CUDA programming. In most cases, cuML's Python API matches the API from scikit-learn.

For large datasets, these GPU-based implementations can complete 10-50x faster than their CPU equivalents.

<p align="center"><img src="https://github.com/rapidsai/cudf/blob/branch-0.13/img/rapids_arrow.png" width="80%"/></p>

[Getting started](https://rapids.ai/start.html)

## Install to /scratch/network

You can request more space in `/home/<YourNetID>` for installing software but because Conda environment are easy to remake we will do the installation on `/scratch/network/<YourNetID>`. Note that this filesystem is not backed up but it provides a vast amount of space (100 GB by default).

```bash
$ ssh adroit
$ cd
$ vim .condarc
```

Edit your file as follows:

```
pkgs_dirs:
 - /scratch/network/<YourNetID>/conda/pkgs
```

Check that it is correct with:

```bash
$ module load anaconda3
$ conda config --show
```

Now perform the installation by copying and pasting the following two lines:

```
$ conda create --prefix /scratch/network/$USER/rapids-env -c rapidsai -c nvidia -c conda-forge \
-c defaults cuml=0.12 python=3.7 cudatoolkit=10.0
```

Or install all components including dask-cudf:

```bash
$ conda create --prefix /scratch/network/jdh4/rapids-env -c rapidsai -c nvidia -c conda-forge \
-c defaults rapids=0.12 python=3.6 cudatoolkit=10.1
```

After the installation one can recover space by deleting the index cache, lock files, unused cache packages, and tarballs:

```
$ conda clean -a
```

## Using cuDF

Note that Rapids requires a GPU with compute capability of 6.0+. This means the K40c GPUs on adroit-h11g4 cannot be used (they are CC 3.5). We mut request the V100 node (CC 7.0). TigerGPU is CC 6.0.

```bash
$ salloc -N 1 -n 1 -t 5 --gres=gpu:tesla_v100:1
$ module load anaconda3
$ conda activate /scratch/network/$USER/rapids-env
$ python
>>> import cudf
>>> s = cudf.Series([1, 2, 3, None, 4])
>>> s
0       1
1       2
2       3
3    null
4       4
dtype: int64
>>> exit()
$ exit
```

See this [guide](https://rapidsai.github.io/projects/cudf/en/0.12.0/10min.html) for a 10-minute introduction to cuDF and Dask-cuDF.

Submitting a job to the Slurm scheduler:

```bash
#!/bin/bash
#SBATCH --job-name=rapids        # create a short name for your job
#SBATCH --nodes=1                # node count
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=1        # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --mem=8G                 # total memory per node
#SBATCH --time=00:05:00          # total run time limit (HH:MM:SS)
#SBATCH --gres=gpu:tesla_v100:1

module purge
module load anaconda3
conda activate /scratch/network/$USER/rapids-env

python rapids.py
```

```python
import cudf
s = cudf.Series([1, 2, 3, None, 4])
print(s)
```

The output is

```
0       1
1       2
2       3
3    null
4       4
dtype: int64
```

## cuDF with Multiple GPUs

```bash
#!/bin/bash
#SBATCH --job-name=rapids        # create a short name for your job
#SBATCH --nodes=1                # node count
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=1        # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --mem=8G                 # total memory per node
#SBATCH --time=00:05:00          # total run time limit (HH:MM:SS)
#SBATCH --gres=gpu:tesla_v100:2

module purge
module load anaconda3
conda activate /scratch/network/jdh4/rapids-env

python rapids.py
```

```python
import cudf
import dask_cudf

df = cudf.DataFrame({'a':list(range(20, 40)), 'b':list(range(20))})

ddf = dask_cudf.from_cudf(df, npartitions=2)
print(ddf.compute())
```

```
     a   b
0   20   0
1   21   1
2   22   2
3   23   3
4   24   4
5   25   5
6   26   6
7   27   7
8   28   8
9   29   9
10  30  10
11  31  11
12  32  12
13  33  13
14  34  14
15  35  15
16  36  16
17  37  17
18  38  18
19  39  19
```

## Machine Learning Example

```bash
$ conda activate /scratch/network/$USER/rapids-env
$ conda install scikit-learn ipywidgets
$ wget https://raw.githubusercontent.com/rapidsai/notebooks/branch-0.13/cuml/svm_demo.ipynb
$ salloc -N 1 -n 1 -t 5 --gres=gpu:tesla_v100:1
```

This [example](https://github.com/rapidsai/notebooks/blob/branch-0.13/cuml/svm_demo.ipynb) can be ran as follows:

```bash
$ ipython svm_demo.py 
CPU times: user 1.99 s, sys: 786 ms, total: 2.78 s
Wall time: 3.55 s
CPU times: user 48.1 s, sys: 258 ms, total: 48.4 s
Wall time: 48.5 s
CPU times: user 196 ms, sys: 27 ms, total: 223 ms
Wall time: 230 ms
CPU times: user 8.91 s, sys: 0 ns, total: 8.91 s
Wall time: 8.93 s
Accuracy: cumlSVC 96.46000000000001%, sklSVC 96.46000000000001%
CPU times: user 851 ms, sys: 68.8 ms, total: 920 ms
Wall time: 942 ms
CPU times: user 477 ms, sys: 318 ms, total: 796 ms
Wall time: 798 ms
CPU times: user 13 s, sys: 70.8 ms, total: 13.1 s
Wall time: 13.1 s
CPU times: user 6.71 ms, sys: 991 Âµs, total: 7.7 ms
Wall time: 7.79 ms
CPU times: user 2.72 s, sys: 0 ns, total: 2.72 s
Wall time: 2.72 s
R2 score: cumlSVR 0.940174859165765, sklSVR 0.9401745984283644
CPU times: user 198 ms, sys: 105 ms, total: 303 ms
Wall time: 304 ms
```

## Useful Links

[Example Notebooks](https://github.com/rapidsai/notebooks)  
[NVIDIA Rapids on GitHub](https://github.com/rapidsai)
